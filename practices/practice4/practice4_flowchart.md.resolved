# Practice4 CUDA Performance Test - Flowchart

This flowchart visualizes the control flow of the CUDA GPU performance benchmarking program for reduction and sorting operations.

## Main Program Flow

```mermaid
flowchart TD
    Start([Start Program]) --> Init[Print Title Message]
    Init --> DefineSize[Define Test Sizes:<br/>10K, 100K, 1M]
    DefineSize --> OpenCSV[Open results.csv for Writing]
    OpenCSV --> WriteHeader[Write CSV Header]
    WriteHeader --> LoopStart{For Each<br/>Array Size}
    
    LoopStart -->|Next Size| PrintSize[Print Current Test Size]
    PrintSize --> AllocMem[Allocate Device Memory:<br/>d_data, d_data_copy]
    AllocMem --> GenRandom[Generate Random Data<br/>on GPU using cuRAND]
    GenRandom --> BackupData[Copy d_data → d_data_copy<br/>Device to Device]
    BackupData --> CopyHost[Copy d_data → h_data<br/>Device to Host]
    CopyHost --> CPUSum[Compute CPU Sum<br/>for Verification]
    CPUSum --> PrintCPUSum[Print CPU Sum Result]
    
    PrintCPUSum --> TestGlobal[Test Reduction<br/>Global Memory]
    TestGlobal --> PrintGlobal[Print Global Memory<br/>Result & Time]
    PrintGlobal --> Restore1[Restore d_data from Backup]
    
    Restore1 --> TestShared[Test Reduction<br/>Shared Memory]
    TestShared --> PrintShared[Print Shared Memory<br/>Result & Time]
    PrintShared --> Restore2[Restore d_data from Backup]
    
    Restore2 --> TestSort[Test Sorting Algorithm]
    TestSort --> PrintSortTime[Print Sorting Time]
    PrintSortTime --> CopyBack[Copy Sorted Data<br/>to Host]
    CopyBack --> Verify[Verify Sorting<br/>Check First 1000 Elements]
    Verify --> PrintVerify[Print Verification Result]
    
    PrintVerify --> WriteCSV[Write Results to CSV:<br/>Size, TimeGlobal, TimeShared, TimeSorting]
    WriteCSV --> Cleanup[Free Memory:<br/>h_data, d_data, d_data_copy]
    Cleanup --> LoopStart
    
    LoopStart -->|Done| CloseFile[Close CSV File]
    CloseFile --> PrintDone[Print Completion Messages]
    PrintDone --> End([End Program])
```

## Reduction Global Memory Flow

```mermaid
flowchart TD
    Start([testReduction<br/>useShared=false]) --> Setup[Setup:<br/>threadsPerBlock=256<br/>Calculate blocksPerGrid]
    Setup --> AllocOut[Allocate d_output Array<br/>Size: blocksPerGrid]
    AllocOut --> AllocHost[Allocate h_output Array]
    AllocHost --> CreateEvents[Create CUDA Events<br/>for Timing]
    CreateEvents --> RecordStart[Record Start Event]
    
    RecordStart --> LaunchGlobal[Launch reductionGlobal Kernel]
    LaunchGlobal --> RecordStop[Record Stop Event]
    RecordStop --> Sync[Synchronize Stop Event]
    Sync --> CalcTime[Calculate Elapsed Time]
    
    CalcTime --> CopyResult[Copy d_output → h_output<br/>Device to Host]
    CopyResult --> CPUFinish[CPU Final Reduction:<br/>Sum all partial results]
    CPUFinish --> Cleanup[Cleanup: Free Memory<br/>Destroy Events]
    Cleanup --> Return[Return Final Sum]
    Return --> End([End])
```

### Reduction Global Kernel Detail

```mermaid
flowchart TD
    KStart([reductionGlobal Kernel]) --> CalcTID[Calculate Global Thread ID<br/>tid = blockIdx.x * blockDim.x + threadIdx.x]
    CalcTID --> CalcStride[Calculate Stride<br/>stride = blockDim.x * gridDim.x]
    CalcStride --> InitSum[Initialize sum = 0.0]
    
    InitSum --> GridLoop{Grid-Stride Loop:<br/>i = tid; i < n}
    GridLoop -->|i < n| AddElement[sum += input'i']
    AddElement --> IncI[i += stride]
    IncI --> GridLoop
    
    GridLoop -->|i >= n| BoundsCheck{tid < n?}
    BoundsCheck -->|Yes| WriteGlobal[Write sum to output'tid'<br/>in Global Memory]
    BoundsCheck -->|No| Skip[Skip Write]
    WriteGlobal --> KEnd([End Kernel])
    Skip --> KEnd
```

## Reduction Shared Memory Flow

```mermaid
flowchart TD
    Start([testReduction<br/>useShared=true]) --> Setup[Setup:<br/>threadsPerBlock=256<br/>Calculate blocksPerGrid]
    Setup --> AllocOut[Allocate d_output Array]
    AllocOut --> AllocHost[Allocate h_output Array]
    AllocHost --> CreateEvents[Create CUDA Events]
    CreateEvents --> RecordStart[Record Start Event]
    
    RecordStart --> LaunchShared[Launch reductionShared Kernel<br/>With Shared Memory]
    LaunchShared --> RecordStop[Record Stop Event]
    RecordStop --> Sync[Synchronize Stop Event]
    Sync --> CalcTime[Calculate Elapsed Time]
    
    CalcTime --> CopyResult[Copy d_output → h_output]
    CopyResult --> CPUFinish[CPU Final Reduction]
    CPUFinish --> Cleanup[Cleanup: Free Memory<br/>Destroy Events]
    Cleanup --> Return[Return Final Sum]
    Return --> End([End])
```

### Reduction Shared Kernel Detail

```mermaid
flowchart TD
    KStart([reductionShared Kernel]) --> DeclareShared[Declare Shared Memory Array]
    DeclareShared --> CalcTID[tid = threadIdx.x<br/>i = blockIdx.x * blockDim.x + threadIdx.x]
    CalcTID --> LoadShared{i < n?}
    LoadShared -->|Yes| LoadData[sdata'tid' = input'i']
    LoadShared -->|No| LoadZero[sdata'tid' = 0.0]
    
    LoadData --> Sync1[__syncthreads]
    LoadZero --> Sync1
    Sync1 --> InitLoop[s = blockDim.x / 2]
    
    InitLoop --> ReductionLoop{s > 0?}
    ReductionLoop -->|Yes| CheckThread{tid < s?}
    CheckThread -->|Yes| AddPair[sdata'tid' += sdata'tid + s']
    CheckThread -->|No| SkipAdd[Skip Addition]
    AddPair --> Sync2[__syncthreads]
    SkipAdd --> Sync2
    Sync2 --> HalveS[s >>= 1]
    HalveS --> ReductionLoop
    
    ReductionLoop -->|s = 0| CheckWrite{tid == 0?}
    CheckWrite -->|Yes| WriteResult[output'blockIdx.x' = sdata'0']
    CheckWrite -->|No| SkipWrite[Skip Write]
    WriteResult --> KEnd([End Kernel])
    SkipWrite --> KEnd
```

## Sorting Flow

```mermaid
flowchart TD
    Start([testSorting Function]) --> Setup[Setup:<br/>chunkSize=512<br/>threadsPerBlock=256<br/>Calculate numChunks]
    Setup --> AllocTemp[Allocate d_temp Buffer]
    AllocTemp --> CreateEvents[Create CUDA Events]
    CreateEvents --> RecordStart[Record Start Event]
    
    RecordStart --> BubbleSort[Launch bubbleSortKernel<br/>Sort All Chunks in Parallel]
    BubbleSort --> SyncBubble[Synchronize Device]
    SyncBubble --> InitMerge[Initialize Merge Phase:<br/>width = chunkSize<br/>d_input = d_data<br/>d_output = d_temp]
    
    InitMerge --> MergeLoop{width < n?}
    MergeLoop -->|Yes| CalcMerges[Calculate numMerges<br/>and sharedSize]
    CalcMerges --> LaunchMerge[Launch mergeSortKernel]
    LaunchMerge --> SyncMerge[Synchronize Device]
    SyncMerge --> SwapBuffers[Swap d_input ↔ d_output<br/>Toggle swapped flag]
    SwapBuffers --> DoubleWidth[width *= 2]
    DoubleWidth --> MergeLoop
    
    MergeLoop -->|width >= n| RecordStop[Record Stop Event]
    RecordStop --> Sync[Synchronize & Calculate Time]
    Sync --> DestroyEvents[Destroy Events]
    DestroyEvents --> CheckSwap{Was swapped?}
    CheckSwap -->|Yes| CopyBack[Copy d_temp → d_data]
    CheckSwap -->|No| NoAction[No Action Needed]
    CopyBack --> FreeTemp[Free d_temp]
    NoAction --> FreeTemp
    FreeTemp --> End([End])
```

### Bubble Sort Kernel Detail

```mermaid
flowchart TD
    KStart([bubbleSortKernel]) --> DeclareShared[Declare Shared Memory]
    DeclareShared --> CalcIdx[blockStart = blockIdx.x * chunkSize<br/>tid = threadIdx.x<br/>globalIdx = blockStart + tid]
    CalcIdx --> LoadCheck{globalIdx < n<br/>AND tid < chunkSize?}
    
    LoadCheck -->|Yes| LoadData[shared'tid' = data'globalIdx']
    LoadCheck -->|No| BoundsCheck{tid < chunkSize?}
    BoundsCheck -->|Yes| LoadMax[shared'tid' = FLT_MAX]
    BoundsCheck -->|No| SkipLoad[Skip Load]
    
    LoadData --> Sync1[__syncthreads]
    LoadMax --> Sync1
    SkipLoad --> Sync1
    Sync1 --> CalcSize[actualSize = min chunkSize, n - blockStart]
    
    CalcSize --> OuterLoop[i = 0]
    OuterLoop --> OuterCheck{i < actualSize - 1?}
    OuterCheck -->|Yes| InnerLoop[j = tid]
    InnerLoop --> InnerCheck{j < actualSize - i - 1?}
    
    InnerCheck -->|Yes| Compare{shared'j' > shared'j+1'?}
    Compare -->|Yes| Swap[Swap:<br/>temp = shared'j'<br/>shared'j' = shared'j+1'<br/>shared'j+1' = temp]
    Compare -->|No| SkipSwap[Skip Swap]
    Swap --> IncJ[j += blockDim.x]
    SkipSwap --> IncJ
    IncJ --> InnerCheck
    
    InnerCheck -->|j >= limit| Sync2[__syncthreads]
    Sync2 --> IncI[i++]
    IncI --> OuterCheck
    
    OuterCheck -->|i >= limit| WriteCheck{globalIdx < n<br/>AND tid < chunkSize?}
    WriteCheck -->|Yes| WriteBack[data'globalIdx' = shared'tid']
    WriteCheck -->|No| SkipWrite[Skip Write]
    WriteBack --> KEnd([End Kernel])
    SkipWrite --> KEnd
```

### Merge Sort Kernel Overview

```mermaid
flowchart TD
    KStart([mergeSortKernel]) --> DeclareShared[Declare Shared Memory]
    DeclareShared --> CalcIdx[Calculate Indices:<br/>tid, blockStart<br/>leftStart, leftEnd<br/>rightStart, rightEnd]
    CalcIdx --> CalcSizes[leftSize, rightSize, totalSize]
    
    CalcSizes --> LoadLeft{tid < leftSize?}
    LoadLeft -->|Yes| LoadL[shared'tid' = input'leftStart + tid']
    LoadLeft -->|No| SkipL[Skip]
    
    LoadL --> LoadRight{tid < rightSize?}
    SkipL --> LoadRight
    LoadRight -->|Yes| LoadR[shared'leftSize + tid' = input'rightStart + tid']
    LoadRight -->|No| SkipR[Skip]
    
    LoadR --> Sync[__syncthreads]
    SkipR --> Sync
    Sync --> MergeLoop[For i = tid to totalSize<br/>Step: blockDim.x]
    
    MergeLoop --> BinarySearch[Binary Search to Find<br/>Position in Merged Array]
    BinarySearch --> DetermineSource{From Left<br/>or Right?}
    DetermineSource -->|Left| TakeLeft[output'blockStart + i' = shared'leftIdx']
    DetermineSource -->|Right| TakeRight[output'blockStart + i' = shared'leftSize + rightIdx']
    
    TakeLeft --> NextI[Next i]
    TakeRight --> NextI
    NextI --> KEnd([End Kernel])
```

## Supporting Functions

### Random Data Generation

```mermaid
flowchart TD
    Start([generateRandomData]) --> CreateGen[Create cuRAND Generator<br/>PSEUDO_DEFAULT]
    CreateGen --> SetSeed[Set Seed = 1234]
    SetSeed --> Generate[Generate n Uniform<br/>Random Floats 0,1]
    Generate --> Destroy[Destroy Generator]
    Destroy --> End([End])
```

### CPU Sum Verification

```mermaid
flowchart TD
    Start([cpuSum Function]) --> Init[sum = 0.0 double]
    Init --> Loop{i < n?}
    Loop -->|Yes| Add[sum += data'i']
    Add --> Inc[i++]
    Inc --> Loop
    Loop -->|No| Cast[Cast sum to float]
    Cast --> Return[Return sum]
    Return --> End([End])
```

## Memory Architecture Overview

```mermaid
graph LR
    subgraph CPU["CPU (Host)"]
        A[h_data<br/>Host Memory]
        B[h_output<br/>Partial Sums]
    end
    
    subgraph GPU["GPU (Device)"]
        C[d_data<br/>Global Memory]
        D[d_data_copy<br/>Backup Copy]
        E[d_output<br/>Results]
        F[d_temp<br/>Sorting Buffer]
        G[Shared Memory<br/>Per Block]
    end
    
    A -.->|cudaMemcpy<br/>D→H| C
    C -.->|cudaMemcpy<br/>H→D| A
    C -->|Backup| D
    E -.->|Results| B
    C <-->|Swap| F
    C -->|Load| G
    G -->|Store| C
```

## Performance Comparison

The program compares three operations across different array sizes:

1. **Reduction (Global Memory)**: Uses only global memory for reduction
2. **Reduction (Shared Memory)**: Uses shared memory for faster intra-block reduction
3. **Sorting**: Hybrid approach with bubble sort for chunks and merge sort for combining

Results are saved to `results.csv` for performance analysis.
