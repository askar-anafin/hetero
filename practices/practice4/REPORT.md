# Отчет по Практической Работе №4: Параллельная обработка данных на GPU

## 1. Введение

В рамках данной работы была реализована программа на CUDA C++ для выполнения двух основных вычислительных задач: редукции (вычисления суммы элементов) и сортировки массива. Главной целью было изучение иерархии памяти GPU и демонстрация преимуществ использования разделяемой памяти (Shared Memory).

## 2. Реализация Задач

### Задание 1: Подготовка данных

Для генерации случайных чисел использовалась библиотека **cuRAND**.

* **Реализация:** Функция `generateRandomData`.
* **Обоснование:** Генерация происходит непосредственно в глобальной памяти видеокарты. Это исключает накладные расходы на копирование огромных массивов с CPU на GPU (по шине PCIe), что критично для производительности при больших объемах данных (1,000,000+ элементов).

### Задание 2: Оптимизация Параллельной Редукции

Были реализованы два ядра для вычисления суммы элементов массива.

#### A. Редукция через Глобальную Память (`reductionGlobal`)

* **Принцип:** Каждый поток суммирует свою часть данных, а затем результат добавляется к глобальному аккумулятору.
* **Особенность реализации:** Для корректности работы при одновременном доступе множества потоков к одной ячейке памяти использовалась операция `atomicAdd`.
* **Недостатки:** Атомарные операции в глобальной памяти вызывают высокую конкуренцию (contention) и сериализацию доступа, а сама глобальная память имеет высокую задержку (~400-800 циклов). Это делает метод значительно медленнее.

#### B. Редукция через Глобальную + Разделяемую Память (`reductionShared`)

* **Принцип:**
    1. Потоки блока загружают данные из глобальной памяти в **Shared Memory** (быстрая память внутри мультипроцессора, аналог L1 кэша).
    2. Производится древовидная редукция внутри разделяемой памяти.
    3. Только один поток от каждого блока записывает результат блока в глобальную память (либо использует атомарную операцию для финальной суммы, либо возвращает частичные суммы для дособирания на хосте/вторым запуском). В нашей реализации используется возврат частичных сумм.
* **Преимущества:** Обращения к Shared Memory в сотни раз быстрее. Количество обращений к "медленной" глобальной памяти сокращается в разы (пропорционально размеру блока).

### Задание 3: Оптимизация Сортировки на GPU

Реализован гибридный алгоритм сортировки слиянием.

#### A. Локальная сортировка (`bubbleSortKernel`)

* **Метод:** Параллельная четно-нечетная сортировка (Odd-Even Transposition Sort) — это параллельная модификация пузырьковой сортировки.
* **Память:** Сортировка происходит полностью в **Shared Memory**.
* **Логика:** Массив разбивается на чанки (по 512 элементов). Каждый блок SM загружает свой чанк, сортирует его автономно без доступа к глобальной памяти, и выгружает обратно.

#### B. Слияние подмассивов

Реализовано два ядра для слияния отсортированных последовательностей, чтобы обойти ограничения по объему памяти:

1. **Shared Memory Merge (`mergeSortKernel`):** Используется для слияния небольших массивов, которые помещаются в Shared Memory (до ~32-48 КБ). Это обеспечивает максимальную скорость благодаря высокой пропускной способности общей памяти.
2. **Global Memory Merge (`mergeSortGlobalKernel`):** Используется как "fallback" для больших массивов, когда размер сливаемых частей превышает лимит Shared Memory. Реализует тот же алгоритм слияния, но читает/пишет напрямую в глобальную память.

**Алгоритм слияния:** Использован подход, где каждый поток вычисляет свою позицию в выходном массиве с помощью бинарного поиска (**Binary Search**) по двум входным массивам. Это позволяет выполнять слияние полностью параллельно.

## 3. Результаты Измерений (Task 4)

Тестирование проводилось на массивах перемнного размера. Ниже приведены актуальные результаты (в миллисекундах):

| Размер (elem) | Global Reduction (ms) | Shared Reduction (ms) | Speedup (Shared vs Global) | Sorting (ms) |
|---------------|-----------------------|-----------------------|----------------------------|--------------|
| 10,000        | 1.283                 | 0.013                 | **~96x**                   | 0.553        |
| 100,000       | 0.334                 | 0.014                 | **~23x**                   | 3.578        |
| 1,000,000     | 0.964                 | 0.080                 | **~12x**                   | 44.139       |

### Анализ графиков и данных

1. **Редукция:**
    * **Shared Memory** значительно быстрее на всех размерах.
    * На 100,000 элементах получен максимальный прирост (**~19x**): Global Memory вариант (0.359 ms) против Shared Memory (0.019 ms).
    * На 1,000,000 элементах прирост стабилизируется на уровне **~11x** (0.920 ms vs 0.081 ms). Это подтверждает, что снижение количества обращений к глобальной памяти критически важно для пропускной способности.

2. **Сортировка:**
    * Сортировка 1 миллиона элементов заняла всего **~41.7 ms**.
    * Линейный рост времени при экспоненциальном росте объема данных (с поправкой на N*logN) показывает эффективность гибридного подхода (локальная сортировка + merge).

## 4. Заключение

В ходе выполнения работы были достигнуты все поставленные цели:

1. Реализована генерация данных на GPU.
2. Разработаны и сравнены два алгоритма редукции: вариант с Shared Memory значительно быстрее.
3. Оптимизирована сортировка: комбинация локальной сортировки в быстрой памяти и глобального слияния позволила обработать большие массивы данных, выходящие за пределы кэша.

Работа продемонстрировала, что грамотное использование иерархии памяти (минимизация доступа к Global Memory и использование Shared Memory как управляемого кэша) является критическим фактором оптимизации CUDA-приложений.
