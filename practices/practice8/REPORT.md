# Отчет по Практической работе 8: Гибридная обработка данных (CPU + GPU)

## 1. Результаты работы

### Описание выполненных заданий

Была реализована программа для обработки массива данных размером **1 000 000** элементов (умножение каждого элемента на 2). Реализованы три подхода:

1. **CPU (OpenMP)**: Параллельная обработка на процессоре.
2. **GPU (CUDA)**: Обработка на графическом процессоре с передачей памяти.
3. **Гибридный режим**: Совместная обработка, где половина массива обрабатывается на CPU, а вторая половина — на GPU одновременно.

### Исходный код

Основные файлы реализации:

- **[main.cu](main.cu)**: Содержит логику замеров времени и вызов функций обработки.
- **[kernels.cu](kernels.cu)**: Реализация CUDA ядер и оберток для вызова GPU.
- **[kernels.cuh](kernels.cuh)**: Заголовочный файл для CUDA функций.

Ключевой фрагмент гибридной обработки (псевдокод):

```cpp
// Запуск GPU части асинхронно
cudaMemcpyAsync(d_data, host_data_part2, ... stream);
multiply_kernel<<<grid, block, 0, stream>>>(d_data, ...);

// Запуск CPU части параллельно
#pragma omp parallel for
for(int i = 0; i < mid; i++) {
    host_data[i] *= 2.0f;
}

// Ожидание завершения GPU
cudaMemcpyAsync(host_data_part2, d_data, ... stream);
cudaStreamSynchronize(stream);
```

### Результаты замеров времени выполнения (N = 1 000 000)

Замеры проводились после "прогрева" CUDA контекста.

| Режим | Время выполнения (мс) |
|---|---|
| **CPU (OpenMP)** | ~1.89 мс |
| **GPU (CUDA)** | ~2.50 мс |
| **Гибридный** | ~1.23 мс |

### Анализ производительности

* **CPU**: Показал хорошее время (1.89 мс) благодаря OpenMP и простоте операции (нет накладных расходов на передачу данных).
- **GPU**: Оказался медленнее CPU (2.50 мс) для данного размера задачи. Это связано с тем, что время передачи данных (PCIe) превышает выигрыш от параллелизма GPU для простой арифметической операции над 1 млн float.
- **Гибридный режим**: Показал лучший результат (**1.23 мс**). Это достигается за счет перекрытия вычислений: пока CPU обрабатывает свою половину данных, GPU (через асинхронные стримы) обрабатывает вторую половину. Итоговое время определяется наиболее медленной из двух параллельных ветвей (в данном случае, обработка на GPU и копирование заняли чуть больше, чем половина работы CPU, но суммарно быстрее последовательного выполнения).

---

## 2. Выводы

### В каких случаях гибридный подход эффективен?

Гибридный подход эффективен, когда:

1. **Задача хорошо делится** на независимые части.
2. **Время обработки на CPU и GPU сопоставимо**. Если одно устройство значительно быстрее другого, гибридный подход может не дать выигрыша из-за накладных расходов на синхронизацию, и лучше всё отдать более быстрому устройству.
3. **Есть возможность перекрытия (Overlap)** передачи данных и вычислений.

### Какие факторы влияют на производительность гибридных вычислений?

1. **Балансировка нагрузки**. Важно правильно разделить данные. Деление 50/50 не всегда оптимально; лучше делить пропорционально вычислительной мощности (например, 30% CPU / 70% GPU).
2. **Накладные расходы PCIe**. Время копирования памяти `Host <-> Device` является узким местом.
3. **Синхронизация**. Ожидание завершения потоков (`cudaStreamSynchronize`) может блокировать CPU, если не использовать асинхронность правильно.

### Как можно оптимизировать передачу данных между CPU и GPU?

1. **Асинхронные копирования**: Использовать `cudaMemcpyAsync` и `cudaStream`.
2. **Pinned Memory (Закрепленная память)**: Использовать `cudaHostAlloc` / `cudaMallocHost` вместо обычного `malloc` / `new`. Это ускоряет передачу данных по шине PCIe.
3. **Zero-Copy Memory**: Для интегрированных GPU или устройств с общей памятью (как NVIDIA Grace Hopper или Tegra), можно использовать память, доступную обоим устройствам без явного копирования.
4. **Скрытие латентности**: Выполнять полезную работу на CPU во время передачи данных на GPU.
